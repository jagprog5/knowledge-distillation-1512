{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-01 11:25:47.611943: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-01 11:25:47.657160: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-01 11:25:47.657839: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-01 11:25:48.376484: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/john/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-11-01 11:25:54.046462: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-11-01 11:25:54.046768: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-11-01 11:25:54.050711: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1309593600 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from typing import Union\n",
    "\n",
    "tf.enable_v2_behavior()\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import csv\n",
    "import os\n",
    "path = os.path.join(os.path.abspath(''), \"mhist_dataset\", \"annotations.csv\")\n",
    "\n",
    "raw_data = []\n",
    "with open(path, 'r') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    next(csv_reader) # skip header row\n",
    "    for row in csv_reader:\n",
    "        raw_data.append(row)\n",
    "\n",
    "# file name (MHIST_zzz.png),HP/SSA,Number of Annotations who selecteed SSA out of 7,train/test\n",
    "\n",
    "raw_train = []\n",
    "raw_test = []\n",
    "\n",
    "# split raw into train and test\n",
    "# also convert the columns to indicate the number of annotators for HP vs SSA\n",
    "\n",
    "for elem in raw_data:\n",
    "    to_add = raw_train if elem[-1] == \"train\" else raw_test\n",
    "    num_ssa = int(elem[2])\n",
    "    num_hp = 7 - num_ssa\n",
    "    num_ssa /= 7 # normalize value\n",
    "    num_hp /= 7\n",
    "    to_add.append([elem[0], num_ssa, num_hp])\n",
    "\n",
    "del raw_data\n",
    "\n",
    "def create_train_test(raw):\n",
    "    x = np.zeros([len(raw),224,224,3], dtype=np.float32)\n",
    "    y = np.zeros([len(raw),2], dtype=np.float32)\n",
    "\n",
    "    for i, elem in enumerate(raw):\n",
    "        file_name = elem[0]\n",
    "        file_path = os.path.join(os.path.abspath(''), \"mhist_dataset\", \"images\", file_name)\n",
    "        image = Image.open(file_path)\n",
    "        image_array = np.array(image, dtype=np.float32) / 255.0\n",
    "        x[i] = image_array\n",
    "\n",
    "        y[i][0] = elem[1]\n",
    "        y[i][1] = elem[2]\n",
    "    \n",
    "    return x,y\n",
    "\n",
    "train_x, train_y = create_train_test(raw_train)\n",
    "test_x, test_y = create_train_test(raw_test)\n",
    "\n",
    "TOTAL_TESTS = test_x.shape[0]\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "INITIAL_EPOCHS = 10\n",
    "FINE_TUNING_EPOCHS = 12\n",
    "TEACHER_LEARNING_RATE=1e-4\n",
    "STUDENT_LEARNING_RATE=1e-3\n",
    "\n",
    "mhist_train = tf.data.Dataset.from_tensor_slices((train_x, train_y)).cache()\n",
    "mhist_train = mhist_train.shuffle(buffer_size=mhist_train.cardinality())\n",
    "mhist_train = mhist_train.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "mhist_test = tf.data.Dataset.from_tensor_slices((test_x, test_y)).cache()\n",
    "mhist_test = mhist_test.batch(BATCH_SIZE)\n",
    "del train_x, train_y, test_x, test_y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the teacher\n",
    "from tensorflow.keras.applications import ResNet50V2\n",
    "# use pretrained weights\n",
    "resnet50v2 = ResNet50V2(include_top=False, weights='imagenet', input_tensor=tf.keras.layers.Input(shape=(224,224,3)))\n",
    "cnn_model = tf.keras.Sequential()\n",
    "cnn_model.add(resnet50v2)\n",
    "cnn_model.add(tf.keras.layers.Flatten())\n",
    "cnn_model.add(tf.keras.layers.Activation('relu'))\n",
    "cnn_model.add(tf.keras.layers.Dense(2)) # no activation (logit output)\n",
    "\n",
    "# here is the student\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "# weights=None means random, don't pre fetch from anywhere\n",
    "mobnetv2 = MobileNetV2(include_top=False, weights=None, input_tensor=tf.keras.layers.Input(shape=(224,224,3)))\n",
    "fc_model = tf.keras.Sequential()\n",
    "fc_model.add(mobnetv2)\n",
    "fc_model.add(tf.keras.layers.Flatten())\n",
    "fc_model.add(tf.keras.layers.Activation('relu'))\n",
    "fc_model.add(tf.keras.layers.Dense(2)) # no activation (logit output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teacher loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def compute_teacher_loss(images, labels):\n",
    "  subclass_logits = cnn_model(images, training=True)\n",
    "  cross_entropy_loss_value = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels, subclass_logits))\n",
    "  return cross_entropy_loss_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@test {\"output\": \"ignore\"}\n",
    "\n",
    "# Hyperparameters for distillation (need to be tuned).\n",
    "ALPHA = 0.5 # task balance between cross-entropy and distillation loss\n",
    "DISTILLATION_TEMPERATURE_ORIGINAL = 4\n",
    "DISTILLATION_TEMPERATURE = DISTILLATION_TEMPERATURE_ORIGINAL #temperature hyperparameter\n",
    "\n",
    "@tf.function\n",
    "def distillation_loss(teacher_logits: tf.Tensor, student_logits: tf.Tensor,\n",
    "                      temperature: Union[float, tf.Tensor]):\n",
    "  soft_targets = tf.nn.softmax(teacher_logits / temperature)\n",
    "\n",
    "  return tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "          soft_targets, student_logits / temperature)) * temperature ** 2\n",
    "\n",
    "@tf.function\n",
    "def compute_student_loss(images, labels):\n",
    "  student_subclass_logits = fc_model(images, training=True)\n",
    "\n",
    "  # Compute subclass distillation loss between student subclass logits and\n",
    "  # softened teacher subclass targets probabilities.\n",
    "\n",
    "  # your code start from here for step 3\n",
    "\n",
    "  teacher_subclass_logits = cnn_model(images, training=False)\n",
    "  distillation_loss_value = distillation_loss(teacher_subclass_logits, student_subclass_logits, DISTILLATION_TEMPERATURE)\n",
    "\n",
    "  # Compute cross-entropy loss with hard targets.\n",
    "\n",
    "  # your code start from here for step 3\n",
    "  cross_entropy_loss_value = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels, student_subclass_logits))\n",
    "\n",
    "  return distillation_loss_value * ALPHA + cross_entropy_loss_value * (1 - ALPHA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# accumulate over batches for f1 score\n",
    "predicted_classes = [] # list of numpy array for each batch. concatenated before use\n",
    "true_classes = []\n",
    "\n",
    "def compute_num_correct(model, images, labels):\n",
    "  \"\"\"Compute number of correctly classified images in a batch.\n",
    "\n",
    "  Args:\n",
    "    model: Instance of tf.keras.Model.\n",
    "    images: Tensor representing a batch of images.\n",
    "    labels: Tensor representing a batch of labels.\n",
    "\n",
    "  Returns:\n",
    "    Number of correctly classified images.\n",
    "  \"\"\"\n",
    "  class_logits = model(images, training=False)\n",
    "\n",
    "  global predicted_classes, true_classes\n",
    "\n",
    "  predicted_classes_batch = tf.math.argmax(class_logits, axis=1)\n",
    "  true_classes_batch = tf.math.argmax(labels, axis=1)\n",
    "\n",
    "  predicted_classes.append(predicted_classes_batch.numpy())\n",
    "  true_classes.append(true_classes_batch.numpy())\n",
    "\n",
    "  return tf.reduce_sum(tf.cast(tf.equal(predicted_classes_batch, true_classes_batch), tf.int32))\n",
    "\n",
    "def train_and_evaluate(model, compute_loss_fn, num_epochs, learning_rate):\n",
    "  \"\"\"Perform training and evaluation for a given model.\n",
    "\n",
    "  Args:\n",
    "    model: Instance of tf.keras.Model.\n",
    "    compute_loss_fn: A function that computes the training loss given the\n",
    "      images, and labels.\n",
    "  \"\"\"\n",
    "\n",
    "  # your code start from here for step 4\n",
    "  optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "  for epoch in range(1, num_epochs + 1):\n",
    "    # Run training.\n",
    "    print('Epoch {}: '.format(epoch), end='')\n",
    "    for images, labels in mhist_train:\n",
    "      with tf.GradientTape() as tape:\n",
    "         # your code start from here for step 4\n",
    "        loss_value = compute_loss_fn(images, labels)\n",
    "      grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "      optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    # Run evaluation.\n",
    "    num_correct = tf.constant(0, dtype=tf.int32)\n",
    "    num_total = TOTAL_TESTS\n",
    "    for images, labels in mhist_test:\n",
    "      # your code start from here for step 4\n",
    "      num = compute_num_correct(model, images, labels)\n",
    "      num_correct += num\n",
    "    \n",
    "    last_accuracy = num_correct / num_total * 100\n",
    "    print(\"Class_accuracy: \" + '{:.2f}%'.format(last_accuracy), end='')\n",
    "\n",
    "    # f1 print out\n",
    "    print(\"f1: \", f1_score(np.concatenate(true_classes), np.concatenate(predicted_classes)))\n",
    "\n",
    "    predicted_classes.clear() # reset for next epoch or end\n",
    "    true_classes.clear()\n",
    "  return last_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-01 11:25:57.414918: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1309593600 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class_accuracy: 68.47%f1:  0.7946666666666666\n",
      "Epoch 2: Class_accuracy: 74.92%f1:  0.8283111422564821\n",
      "Epoch 3: Class_accuracy: 79.53%f1:  0.8514115898959881\n",
      "Epoch 4: Class_accuracy: 79.63%f1:  0.8539985326485693\n",
      "Epoch 5: Class_accuracy: 81.17%f1:  0.8566978193146417\n",
      "Epoch 6: Class_accuracy: 79.63%f1:  0.8531365313653136\n",
      "Epoch 7: Class_accuracy: 81.17%f1:  0.8546603475513428\n",
      "Epoch 8: Class_accuracy: 83.32%f1:  0.8743253662297609\n",
      "Epoch 9: Class_accuracy: 82.50%f1:  0.8671328671328672\n",
      "Epoch 10: Class_accuracy: 81.37%f1:  0.8639760837070253\n"
     ]
    }
   ],
   "source": [
    "drop = train_and_evaluate(cnn_model, compute_teacher_loss, INITIAL_EPOCHS, TEACHER_LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Class_accuracy: 63.15%f1:  0.7741530740276036\n",
      "Epoch 2: Class_accuracy: 36.85%f1:  0.0\n",
      "Epoch 3: Class_accuracy: 63.15%f1:  0.7741530740276036\n",
      "Epoch 4: "
     ]
    }
   ],
   "source": [
    "drop = train_and_evaluate(fc_model, compute_student_loss, INITIAL_EPOCHS, STUDENT_LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train student from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train_and_evaluate() missing 2 required positional arguments: 'num_epochs' and 'learning_rate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/john/repos/school/ECE1512-A2/Project_A_Supp/Task2.ipynb Cell 15\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/john/repos/school/ECE1512-A2/Project_A_Supp/Task2.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m   cross_entropy_loss_value \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mreduce_mean(tf\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39msoftmax_cross_entropy_with_logits(labels, student_subclass_logits))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/john/repos/school/ECE1512-A2/Project_A_Supp/Task2.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m cross_entropy_loss_value\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/john/repos/school/ECE1512-A2/Project_A_Supp/Task2.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m drop \u001b[39m=\u001b[39m train_and_evaluate(fc_model_no_distillation, compute_plain_cross_entropy_loss)\n",
      "\u001b[0;31mTypeError\u001b[0m: train_and_evaluate() missing 2 required positional arguments: 'num_epochs' and 'learning_rate'"
     ]
    }
   ],
   "source": [
    "# Build fully connected student.\n",
    "fc_model_no_distillation = tf.keras.models.clone_model(fc_model) # checked online. this does not copy the weights. fresh start\n",
    "\n",
    "# your code start from here for step 7\n",
    "\n",
    "#@test {\"output\": \"ignore\"}\n",
    "def compute_plain_cross_entropy_loss(images, labels):\n",
    "  \"\"\"Compute plain loss for given images and labels.\n",
    "\n",
    "  For fair comparison and convenience, this function also performs a\n",
    "  LogSumExp over subclasses, but does not perform subclass distillation.\n",
    "\n",
    "  Args:\n",
    "    images: Tensor representing a batch of images.\n",
    "    labels: Tensor representing a batch of labels.\n",
    "\n",
    "  Returns:\n",
    "    Scalar loss Tensor.\n",
    "  \"\"\"\n",
    "  # your code start from here for step 7\n",
    "\n",
    "  student_subclass_logits = fc_model_no_distillation(images, training=True)\n",
    "  cross_entropy_loss_value = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels, student_subclass_logits))\n",
    "  return cross_entropy_loss_value\n",
    "\n",
    "drop = train_and_evaluate(fc_model_no_distillation, compute_plain_cross_entropy_loss, INITIAL_EPOCHS, STUDENT_LEARNING_RATE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
