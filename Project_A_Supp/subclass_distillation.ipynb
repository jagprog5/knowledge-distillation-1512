{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preamble copied from Task1.ipynb\n",
    "\n",
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from typing import Union\n",
    "\n",
    "tf.enable_v2_behavior()\n",
    "\n",
    "builder = tfds.builder('mnist')\n",
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 200\n",
    "NUM_CLASSES = 10  # 10 total classes.\n",
    "\n",
    "# number of subclasses per class\n",
    "SUBCLASSES = 2\n",
    "BETA = 0.7 # hyperparameter regarding auxillary loss\n",
    "AUXILLIARY_TEMPERATURE = 4\n",
    "\n",
    "# Load train and test splits.\n",
    "def preprocess(x):\n",
    "  image = tf.image.convert_image_dtype(x['image'], tf.float32)\n",
    "  subclass_labels = tf.one_hot(x['label'], builder.info.features['label'].num_classes)\n",
    "  return image, subclass_labels\n",
    "  \n",
    "mnist_train = tfds.load('mnist', split='train', shuffle_files=False).cache()\n",
    "mnist_train = mnist_train.map(preprocess)\n",
    "mnist_train = mnist_train.shuffle(builder.info.splits['train'].num_examples)\n",
    "mnist_train = mnist_train.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "mnist_test = tfds.load('mnist', split='test').cache()\n",
    "mnist_test = mnist_test.map(preprocess).batch(BATCH_SIZE)\n",
    "\n",
    "# this layer compresses the subclasses to the classes via a summation\n",
    "class SubclassCollapseLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, n, **kwargs):\n",
    "        self.n = n\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    def call(self, vals):\n",
    "        # difficult to reshape a tensor with unknown first dimension\n",
    "        vals = tf.expand_dims(vals, axis=-1)\n",
    "        shape = [tf.shape(vals)[k] for k in range(len(vals.shape))]\n",
    "        shape[-1] = 2\n",
    "        shape[-2] = -1 # infer\n",
    "        vals = tf.reshape(vals, shape=shape)\n",
    "        vals = tf.reduce_sum(vals, axis=-1)\n",
    "        return vals\n",
    "\n",
    "cnn_input = tf.keras.Input(shape=(28, 28, 1), dtype=tf.float32)\n",
    "\n",
    "# 3x3 convolution\n",
    "cnn_layer0 = tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\")(cnn_input)\n",
    "# 2x2 spatial pool\n",
    "cnn_layer1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(cnn_layer0)\n",
    "# another convolution\n",
    "cnn_layer2 = tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\")(cnn_layer1)\n",
    "# another pool\n",
    "cnn_layer3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(cnn_layer2)\n",
    "# flatten\n",
    "cnn_layer4 = tf.keras.layers.Flatten()(cnn_layer3)\n",
    "# dropout\n",
    "cnn_layer5 = tf.keras.layers.Dropout(0.5)(cnn_layer4)\n",
    "# fully connected\n",
    "cnn_layer6 = tf.keras.layers.Dense(128, activation=\"relu\")(cnn_layer5)\n",
    "# dropout again\n",
    "cnn_layer7 = tf.keras.layers.Dropout(0.5)(cnn_layer6)\n",
    "\n",
    "cnn_layer8 = tf.keras.layers.Dense(NUM_CLASSES * SUBCLASSES)(cnn_layer7) # no activation (logit output)\n",
    "\n",
    "cnn_layer9 = SubclassCollapseLayer(SUBCLASSES)(cnn_layer8) # still logits\n",
    "\n",
    "cnn_model = tf.keras.Model(inputs=cnn_input, outputs=[cnn_layer8, cnn_layer9])\n",
    "\n",
    "# student model\n",
    "fc_input = tf.keras.Input(shape=(28, 28, 1), dtype=tf.float32)\n",
    "fc_layer0 = tf.keras.layers.Flatten()(fc_input)\n",
    "fc_layer1 = tf.keras.layers.Dense(784, activation=\"relu\")(fc_layer0)\n",
    "fc_layer2 = tf.keras.layers.Dense(784, activation=\"relu\")(fc_layer1)\n",
    "fc_layer3 = tf.keras.layers.Dense(NUM_CLASSES * SUBCLASSES)(fc_layer2) # no activation (logit output)\n",
    "fc_layer4 = SubclassCollapseLayer(SUBCLASSES)(fc_layer3) # still logits\n",
    "fc_model = tf.keras.Model(inputs=fc_input, outputs=[fc_layer3, fc_layer4])\n",
    "\n",
    "# ================================================================================================\n",
    "\n",
    "def compute_subclass_loss(subclass_logits):\n",
    "    # using auxiliary loss equation described in the paper\n",
    "\n",
    "    # input is 256 x (c x 10)\n",
    "    n = tf.constant(subclass_logits.shape[0], dtype=float)\n",
    "    T = AUXILLIARY_TEMPERATURE\n",
    "\n",
    "    # normalized it. as described in the paper, mean 0 and var 1\n",
    "    mean, variance = tf.nn.moments(subclass_logits, axes=[1], keepdims=True)\n",
    "    vals = (subclass_logits - mean) / tf.sqrt(variance + 1e-8) # normalized_subclass_logits\n",
    "    \n",
    "    # I learned einstein summation notation for this assignment. very cool very good\n",
    "    term = tf.reduce_sum(tf.math.log(tf.exp(tf.einsum('ik,jk->ij', vals, vals) / T)))\n",
    "\n",
    "    ret = (1 / n) * term - (1 / T) - tf.math.log(n)\n",
    "    return ret\n",
    "\n",
    "@tf.function\n",
    "def compute_teacher_loss(images, labels):\n",
    "  subclass_logits, logits = cnn_model(images, training=True)\n",
    "\n",
    "  typical_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels, logits))\n",
    "  auxiliary_loss = compute_subclass_loss(subclass_logits)\n",
    "\n",
    "  return typical_loss + BETA * auxiliary_loss\n",
    "\n",
    "# ================================================================================================\n",
    "\n",
    "@tf.function\n",
    "def compute_num_correct(model, images, labels):\n",
    "  \"\"\"Compute number of correctly classified images in a batch.\n",
    "\n",
    "  Args:\n",
    "    model: Instance of tf.keras.Model.\n",
    "    images: Tensor representing a batch of images.\n",
    "    labels: Tensor representing a batch of labels.\n",
    "\n",
    "  Returns:\n",
    "    Number of correctly classified images.\n",
    "  \"\"\"\n",
    "  class_logits = model(images, training=False)[1]\n",
    "  return tf.reduce_sum(\n",
    "      tf.cast(tf.math.equal(tf.argmax(class_logits, -1), tf.argmax(labels, -1)),\n",
    "              tf.int32)), tf.argmax(class_logits, -1), tf.argmax(labels, -1)\n",
    "\n",
    "def train_and_evaluate(model, compute_loss_fn):\n",
    "  \"\"\"Perform training and evaluation for a given model.\n",
    "\n",
    "  Args:\n",
    "    model: Instance of tf.keras.Model.\n",
    "    compute_loss_fn: A function that computes the training loss given the\n",
    "      images, and labels.\n",
    "  \"\"\"\n",
    "\n",
    "  # your code start from here for step 4\n",
    "  optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "  for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    # Run training.\n",
    "    do_print = epoch % 5 == 0\n",
    "    if do_print:\n",
    "      print('Epoch {}: '.format(epoch), end='')\n",
    "    for images, labels in mnist_train:\n",
    "      with tf.GradientTape() as tape:\n",
    "         # your code start from here for step 4\n",
    "        loss_value = compute_loss_fn(images, labels)\n",
    "      grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "      optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    # Run evaluation.\n",
    "    num_correct = tf.constant(0, dtype=tf.int32)\n",
    "    num_total = builder.info.splits['test'].num_examples\n",
    "    for images, labels in mnist_test:\n",
    "      # your code start from here for step 4\n",
    "      num_correct += tf.reduce_sum(compute_num_correct(model, images, labels)[0])\n",
    "    \n",
    "    last_accuracy = num_correct / num_total * 100\n",
    "    if do_print:\n",
    "      print(\"Class_accuracy: \" + '{:.2f}%'.format(last_accuracy))\n",
    "  return last_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Class_accuracy: 27.25%\n",
      "Epoch 10: Class_accuracy: 33.73%\n",
      "Epoch 15: Class_accuracy: 46.05%\n",
      "Epoch 20: Class_accuracy: 48.10%\n",
      "Epoch 25: Class_accuracy: 58.72%\n",
      "Epoch 30: Class_accuracy: 65.64%\n",
      "Epoch 35: Class_accuracy: 77.02%\n",
      "Epoch 40: Class_accuracy: 80.96%\n",
      "Epoch 45: Class_accuracy: 85.88%\n",
      "Epoch 50: Class_accuracy: 90.19%\n",
      "Epoch 55: Class_accuracy: 93.06%\n",
      "Epoch 60: Class_accuracy: 93.83%\n",
      "Epoch 65: Class_accuracy: 94.43%\n",
      "Epoch 70: Class_accuracy: 95.22%\n",
      "Epoch 75: Class_accuracy: 95.46%\n",
      "Epoch 80: Class_accuracy: 95.80%\n",
      "Epoch 85: Class_accuracy: 96.04%\n",
      "Epoch 90: Class_accuracy: 96.51%\n",
      "Epoch 95: Class_accuracy: 96.64%\n",
      "Epoch 100: Class_accuracy: 96.62%\n",
      "Epoch 105: Class_accuracy: 96.72%\n",
      "Epoch 110: Class_accuracy: 96.70%\n",
      "Epoch 115: Class_accuracy: 96.72%\n",
      "Epoch 120: Class_accuracy: 96.92%\n",
      "Epoch 125: Class_accuracy: 97.00%\n",
      "Epoch 130: Class_accuracy: 96.73%\n",
      "Epoch 135: Class_accuracy: 97.02%\n",
      "Epoch 140: Class_accuracy: 97.05%\n",
      "Epoch 145: Class_accuracy: 97.04%\n",
      "Epoch 150: Class_accuracy: 97.11%\n",
      "Epoch 155: Class_accuracy: 97.03%\n",
      "Epoch 160: Class_accuracy: 97.10%\n",
      "Epoch 165: Class_accuracy: 97.23%\n",
      "Epoch 170: Class_accuracy: 97.20%\n",
      "Epoch 175: Class_accuracy: 97.35%\n",
      "Epoch 180: Class_accuracy: 97.55%\n",
      "Epoch 185: Class_accuracy: 97.45%\n",
      "Epoch 190: Class_accuracy: 97.48%\n",
      "Epoch 195: Class_accuracy: 97.60%\n",
      "Epoch 200: Class_accuracy: 97.29%\n"
     ]
    }
   ],
   "source": [
    "drop = train_and_evaluate(cnn_model, compute_teacher_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.552832026923353 6.585251884982124\n",
      "0.16282136076192422\n"
     ]
    }
   ],
   "source": [
    "# checking that the properties hold for the subclass weights\n",
    "\n",
    "# this is the dense layer weights that go into creating the \n",
    "weights = cnn_model.layers[-2].get_weights()[0] # at 0 since there are two outputs in the graph\n",
    "# 128 x 20 output weights\n",
    "\n",
    "# n dimensional length\n",
    "first_subclass_total_length = 0\n",
    "second_subclass_total_length = 0\n",
    "\n",
    "dot_pr = 0\n",
    "\n",
    "for i in range(len(weights)):\n",
    "    for j in range(0, len(weights[0]), 2):\n",
    "        first_subclass_total_length += weights[i][j] ** 2\n",
    "        second_subclass_total_length += weights[i][j+1] ** 2\n",
    "        dot_pr += weights[i][j] * weights[i][j+1]\n",
    "\n",
    "first_subclass_total_length **= 0.5\n",
    "second_subclass_total_length **= 0.5\n",
    "print(first_subclass_total_length, second_subclass_total_length)\n",
    "print(dot_pr / (first_subclass_total_length * second_subclass_total_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for distillation (need to be tuned).\n",
    "ALPHA = 0.5 # task balance between cross-entropy and distillation loss\n",
    "DISTILLATION_TEMPERATURE_ORIGINAL = 4\n",
    "DISTILLATION_TEMPERATURE = DISTILLATION_TEMPERATURE_ORIGINAL #temperature hyperparameter\n",
    "\n",
    "# ================================================================================================\n",
    "\n",
    "# distillation loss uses only the subclass logits\n",
    "def distillation_loss(teacher_subclass_logits: tf.Tensor, student_subclass_logits: tf.Tensor,\n",
    "                      temperature: Union[float, tf.Tensor]):\n",
    "  # soften the teacher's logits\n",
    "  soft_targets = tf.nn.softmax(teacher_subclass_logits / temperature)\n",
    "\n",
    "  return tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "          soft_targets, student_subclass_logits / temperature)) * temperature ** 2\n",
    "\n",
    "def compute_student_loss(images, labels):\n",
    "  student_subclass_logits, student_logits = fc_model(images, training=True)\n",
    "  teacher_subclass_logits, teacher_logits = cnn_model(images, training=False)\n",
    "\n",
    "  distillation_loss_value = distillation_loss(teacher_subclass_logits, student_subclass_logits, DISTILLATION_TEMPERATURE)\n",
    "\n",
    "  cross_entropy_loss_value = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels, student_logits))\n",
    "\n",
    "  return distillation_loss_value * ALPHA + cross_entropy_loss_value * (1 - ALPHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Class_accuracy: 98.58%\n",
      "Epoch 10: Class_accuracy: 98.54%\n",
      "Epoch 15: Class_accuracy: 98.57%\n",
      "Epoch 20: Class_accuracy: 98.49%\n",
      "Epoch 25: Class_accuracy: 98.51%\n",
      "Epoch 30: Class_accuracy: 98.55%\n",
      "Epoch 35: Class_accuracy: 98.51%\n",
      "Epoch 40: Class_accuracy: 98.62%\n",
      "Epoch 45: Class_accuracy: 98.53%\n",
      "Epoch 50: Class_accuracy: 98.50%\n",
      "Epoch 55: Class_accuracy: 98.52%\n",
      "Epoch 60: Class_accuracy: 98.55%\n",
      "Epoch 65: Class_accuracy: 98.42%\n",
      "Epoch 70: Class_accuracy: 98.52%\n",
      "Epoch 75: Class_accuracy: 98.53%\n",
      "Epoch 80: Class_accuracy: 98.51%\n",
      "Epoch 85: Class_accuracy: 98.49%\n",
      "Epoch 90: Class_accuracy: 98.51%\n",
      "Epoch 95: Class_accuracy: 98.53%\n",
      "Epoch 100: Class_accuracy: 98.51%\n",
      "Epoch 105: Class_accuracy: 98.56%\n",
      "Epoch 110: Class_accuracy: 98.58%\n",
      "Epoch 115: Class_accuracy: 98.49%\n",
      "Epoch 120: Class_accuracy: 98.46%\n",
      "Epoch 125: Class_accuracy: 98.44%\n",
      "Epoch 130: Class_accuracy: 98.55%\n",
      "Epoch 135: Class_accuracy: 98.51%\n",
      "Epoch 140: Class_accuracy: 98.49%\n",
      "Epoch 145: Class_accuracy: 98.51%\n",
      "Epoch 150: Class_accuracy: 98.48%\n",
      "Epoch 155: Class_accuracy: 98.52%\n",
      "Epoch 160: Class_accuracy: 98.51%\n",
      "Epoch 165: Class_accuracy: 98.47%\n",
      "Epoch 170: Class_accuracy: 98.50%\n",
      "Epoch 175: Class_accuracy: 98.53%\n",
      "Epoch 180: Class_accuracy: 98.46%\n",
      "Epoch 185: Class_accuracy: 98.54%\n",
      "Epoch 190: Class_accuracy: 98.48%\n",
      "Epoch 195: Class_accuracy: 98.49%\n",
      "Epoch 200: Class_accuracy: 98.45%\n"
     ]
    }
   ],
   "source": [
    "drop = train_and_evaluate(fc_model, compute_student_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
